import os
import requests
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import pyreadstat

# Base URLs and folders
BASE_URL = "https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Examination"
XPT_BASE_FOLDER = "/Users/isabella/Library/Mobile Documents/com~apple~CloudDocs/Graduated/UCL EvidENT/DOWNLOADED DATA/XPT DATA"
CSV_BASE_FOLDER = "/Users/isabella/Library/Mobile Documents/com~apple~CloudDocs/Graduated/UCL EvidENT/DOWNLOADED DATA/CSV DATA"

# Create folders if they don't exist
os.makedirs(XPT_BASE_FOLDER, exist_ok=True)
os.makedirs(CSV_BASE_FOLDER, exist_ok=True)

# Datasets to download
datasets = [
    "Cardiovascular Fitness",
    "Ophthalmology - Frequency Doubling Technology",
    "Physical Activity Monitor",
    "Vision"
]

# Fetch and parse page
print("Fetching NHANES Examination page...")
resp = requests.get(BASE_URL)
soup = BeautifulSoup(resp.text, "html.parser")

# Find all tables
table = soup.find("table")
rows = table.find_all("tr")

# Extract header and column indices
header = [th.get_text(strip=True) for th in rows[0].find_all("th")]
data_name_idx = header.index("Data File Name")
data_file_idx = header.index("Data File")

print(f"Detected columns: {header}")

# Process each row
for row in rows[1:]:
    cols = row.find_all("td")
    if not cols:
        continue

    dataset_name = cols[data_name_idx].get_text(strip=True)
    if dataset_name not in datasets:
        continue

    print(f"\nüîç Processing dataset: {dataset_name}")

    # Create subfolders
    xpt_folder = os.path.join(XPT_BASE_FOLDER, dataset_name)
    csv_folder = os.path.join(CSV_BASE_FOLDER, dataset_name)
    os.makedirs(xpt_folder, exist_ok=True)
    os.makedirs(csv_folder, exist_ok=True)

    # Find XPT links in the "Data File" column
    links = cols[data_file_idx].find_all("a")
    for link in links:
        href = link.get("href")
        if not href or not href.lower().endswith(".xpt"):
            continue

        # Resolve relative URL
        xpt_url = urljoin(BASE_URL, href)
        filename = os.path.basename(href)
        xpt_path = os.path.join(xpt_folder, filename)
        csv_filename = filename.replace(".xpt", ".csv")
        csv_path = os.path.join(csv_folder, csv_filename)

        # Download XPT if it doesn't exist
        if not os.path.exists(xpt_path):
            print(f"Downloading {filename}...")
            r = requests.get(xpt_url, stream=True)
            r.raise_for_status()
            with open(xpt_path, "wb") as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
        else:
            print(f"Already downloaded: {filename}")

        # Convert to CSV
        if not os.path.exists(csv_path):
            try:
                df, meta = pyreadstat.read_xport(xpt_path)
                df.to_csv(csv_path, index=False, encoding="utf-8")
                print(f"‚úÖ Converted to CSV: {csv_filename}")
            except Exception as e:
                print(f"‚ùå Failed to convert {filename}: {e}")
        else:
            print(f"CSV already exists: {csv_filename}")

print("\nüéâ All datasets processed!")
